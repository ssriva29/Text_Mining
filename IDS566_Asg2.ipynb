{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMXyzKh7qgLV"
   },
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMykoe_53uAh"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as nm\n",
    "import nltk\n",
    "import collections\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import *  \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 546,
     "status": "ok",
     "timestamp": 1582138664192,
     "user": {
      "displayName": "shubhangi srivastava",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDOH0h-cwBvzv3AHuKWWQdg6Csr1NsYduKRItunaQ=s64",
      "userId": "03789907900117995919"
     },
     "user_tz": 360
    },
    "id": "aG2VneN33zWY",
    "outputId": "6082137f-1dd4-4667-9c0e-99c57ddd22ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shubhangisrivastava/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/shubhangisrivastava/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kSrfNNDHqpHE"
   },
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 561,
     "status": "ok",
     "timestamp": 1582138666906,
     "user": {
      "displayName": "shubhangi srivastava",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDOH0h-cwBvzv3AHuKWWQdg6Csr1NsYduKRItunaQ=s64",
      "userId": "03789907900117995919"
     },
     "user_tz": 360
    },
    "id": "oOFJ6u3hdX8h",
    "outputId": "42540081-4f2d-47df-d9d6-6110c4e87327"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ItemID  Sentiment SentimentSource  \\\n",
      "0           1          0    Sentiment140   \n",
      "1           2          0    Sentiment140   \n",
      "2           3          1    Sentiment140   \n",
      "3           4          0    Sentiment140   \n",
      "4           5          0    Sentiment140   \n",
      "...       ...        ...             ...   \n",
      "89984   89996          1    Sentiment140   \n",
      "89985   89997          1    Sentiment140   \n",
      "89986   89998          1    Sentiment140   \n",
      "89987   89999          0    Sentiment140   \n",
      "89988   90000          0    Sentiment140   \n",
      "\n",
      "                                           SentimentText  \n",
      "0                           is so sad for my APL frie...  \n",
      "1                         I missed the New Moon trail...  \n",
      "2                                omg its already 7:30 :O  \n",
      "3                .. Omgaga. Im sooo  im gunna CRy. I'...  \n",
      "4               i think mi bf is cheating on me!!!   ...  \n",
      "...                                                  ...  \n",
      "89984  @clevercatsknit Re: gnome hat. Was the problem...  \n",
      "89985  @clevercatsknit Saw Linnes Bakery but thought ...  \n",
      "89986                 @cleverdaisies I would LOVE to!!!   \n",
      "89987                          @cleverick evidently not   \n",
      "89988  @cleverindie This spine thing sounds no good  ...  \n",
      "\n",
      "[89989 rows x 4 columns]\n",
      "      ItemID  Sentiment SentimentSource  \\\n",
      "0      90001          1    Sentiment140   \n",
      "1      90002          1    Sentiment140   \n",
      "2      90003          1    Sentiment140   \n",
      "3      90004          1    Sentiment140   \n",
      "4      90005          0    Sentiment140   \n",
      "...      ...        ...             ...   \n",
      "9995   99996          0    Sentiment140   \n",
      "9996   99997          1    Sentiment140   \n",
      "9997   99998          0    Sentiment140   \n",
      "9998   99999          1    Sentiment140   \n",
      "9999  100000          1    Sentiment140   \n",
      "\n",
      "                                          SentimentText  \n",
      "0     @CleverMonkeys but AFTER you hit up the Magnol...  \n",
      "1     @CleverMonkeys only seen a boot on a car in mo...  \n",
      "2     @clevertitania Good morning. We have rain and ...  \n",
      "3               @clewis4u91 so glad i'm not there then   \n",
      "4                  @ClexIsEpic I MSS YOU!    sad cookie  \n",
      "...                                                 ...  \n",
      "9995  @Cupcake  seems like a repeating problem   hop...  \n",
      "9996  @cupcake__ arrrr we both replied to each other...  \n",
      "9997                     @CuPcAkE_2120 ya i thought so   \n",
      "9998  @Cupcake_Dollie Yes. Yes. I'm glad you had mor...  \n",
      "9999                    @cupcake_kayla haha yes you do   \n",
      "\n",
      "[10000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./train.txt', header=None,names= [\"ItemID\", \"Sentiment\", \"SentimentSource\", \"SentimentText\"],\n",
    "                 skiprows=1)\n",
    "print(df)\n",
    "\n",
    "test_df = pd.read_csv('./test.txt', header=None,names= [\"ItemID\", \"Sentiment\", \"SentimentSource\", \"SentimentText\"],\n",
    "                 skiprows=1)\n",
    "\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOxuWFHqq8b-"
   },
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vO9ZzovAq-6D"
   },
   "outputs": [],
   "source": [
    "words_to_remove = stopwords.words('english')\n",
    "puncs = list(string.punctuation)\n",
    "words_to_remove.extend(puncs)\n",
    "words_to_remove.extend(['``','\"\"',\"''\",\"...\",\"\\'\",'\\\"',\"\\t\",\"\\b\",\"\\r\",\"\\f\",\"\\n\"])\n",
    "to_strip = string.punctuation+\".\"+\"\\'\"+'\\\"'+\"\\t\"+\"\\b\"+\"\\r\"+\"\\f\"+\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XA9M5Qn8dP9e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tokens(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens = [word.lower().strip(to_strip) for word in tokens if word.lower() not in set(words_to_remove)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BfW3yMna0lOv"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zB7RCP0rBlz"
   },
   "outputs": [],
   "source": [
    "df[\"SentimentText\"] = df[\"SentimentText\"].map(lambda x: get_tokens(sentence = x))\n",
    "test_df[\"SentimentText\"] = test_df[\"SentimentText\"].map(lambda x: get_tokens(sentence = x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1582138704898,
     "user": {
      "displayName": "shubhangi srivastava",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDOH0h-cwBvzv3AHuKWWQdg6Csr1NsYduKRItunaQ=s64",
      "userId": "03789907900117995919"
     },
     "user_tz": 360
    },
    "id": "9QLuuAA7rK7k",
    "outputId": "9f48fd1f-e376-487e-cf93-b0b26d08082f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ItemID  Sentiment SentimentSource  \\\n",
      "0           1          0    Sentiment140   \n",
      "1           2          0    Sentiment140   \n",
      "2           3          1    Sentiment140   \n",
      "3           4          0    Sentiment140   \n",
      "4           5          0    Sentiment140   \n",
      "...       ...        ...             ...   \n",
      "89984   89996          1    Sentiment140   \n",
      "89985   89997          1    Sentiment140   \n",
      "89986   89998          1    Sentiment140   \n",
      "89987   89999          0    Sentiment140   \n",
      "89988   90000          0    Sentiment140   \n",
      "\n",
      "                                           SentimentText  \n",
      "0                                     [sad, apl, friend]  \n",
      "1                           [missed, new, moon, trailer]  \n",
      "2                                   [omg, already, 7:30]  \n",
      "3      [, omgaga, im, sooo, im, gunna, cry, ve, denti...  \n",
      "4                         [think, mi, bf, cheating, t_t]  \n",
      "...                                                  ...  \n",
      "89984  [clevercatsknit, gnome, hat, problem, finished...  \n",
      "89985  [clevercatsknit, saw, linnes, bakery, thought,...  \n",
      "89986                       [cleverdaisies, would, love]  \n",
      "89987                             [cleverick, evidently]  \n",
      "89988  [cleverindie, spine, thing, sounds, good, back...  \n",
      "\n",
      "[89989 rows x 4 columns]\n",
      "      ItemID  Sentiment SentimentSource  \\\n",
      "0      90001          1    Sentiment140   \n",
      "1      90002          1    Sentiment140   \n",
      "2      90003          1    Sentiment140   \n",
      "3      90004          1    Sentiment140   \n",
      "4      90005          0    Sentiment140   \n",
      "...      ...        ...             ...   \n",
      "9995   99996          0    Sentiment140   \n",
      "9996   99997          1    Sentiment140   \n",
      "9997   99998          0    Sentiment140   \n",
      "9998   99999          1    Sentiment140   \n",
      "9999  100000          1    Sentiment140   \n",
      "\n",
      "                                          SentimentText  \n",
      "0     [clevermonkeys, hit, magnolia, see, anvil, sto...  \n",
      "1     [clevermonkeys, seen, boot, car, movies, goodt...  \n",
      "2         [clevertitania, good, morning, rain, thunder]  \n",
      "3                                 [clewis4u91, glad, m]  \n",
      "4                        [clexisepic, mss, sad, cookie]  \n",
      "...                                                 ...  \n",
      "9995  [cupcake, seems, like, repeating, problem, hop...  \n",
      "9996  [cupcake, arrrr, replied, different, tweets, t...  \n",
      "9997                        [cupcake_2120, ya, thought]  \n",
      "9998           [cupcake_dollie, yes, yes, m, glad, fun]  \n",
      "9999                         [cupcake_kayla, haha, yes]  \n",
      "\n",
      "[10000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F08mqcXTi8mR"
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jsXE4dkOjAMT"
   },
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "df[\"SentimentText\"] = df[\"SentimentText\"].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "test_df[\"SentimentText\"] = test_df[\"SentimentText\"].apply(lambda x: [stemmer.stem(i) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1582138723816,
     "user": {
      "displayName": "shubhangi srivastava",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDOH0h-cwBvzv3AHuKWWQdg6Csr1NsYduKRItunaQ=s64",
      "userId": "03789907900117995919"
     },
     "user_tz": 360
    },
    "id": "Xw7oQ2nJ2-qc",
    "outputId": "1242be6f-67c0-4f7f-e2bf-6af4f3931d65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                       [sad, apl, friend]\n",
      "1                               [miss, new, moon, trailer]\n",
      "2                                     [omg, alreadi, 7:30]\n",
      "3        [, omgaga, im, sooo, im, gunna, cri, ve, denti...\n",
      "4                              [think, mi, bf, cheat, t_t]\n",
      "                               ...                        \n",
      "89984    [clevercatsknit, gnome, hat, problem, finish, ...\n",
      "89985    [clevercatsknit, saw, linn, bakeri, thought, v...\n",
      "89986                           [cleverdaisi, would, love]\n",
      "89987                                    [cleverick, evid]\n",
      "89988    [cleverindi, spine, thing, sound, good, back, ...\n",
      "Name: SentimentText, Length: 89989, dtype: object\n",
      "0       [clevermonkey, hit, magnolia, see, anvil, stor...\n",
      "1       [clevermonkey, seen, boot, car, movi, goodth, ...\n",
      "2              [clevertitania, good, morn, rain, thunder]\n",
      "3                                   [clewis4u91, glad, m]\n",
      "4                             [clexisep, mss, sad, cooki]\n",
      "                              ...                        \n",
      "9995    [cupcak, seem, like, repeat, problem, hope, re...\n",
      "9996    [cupcak, arrrr, repli, differ, tweet, time, ll...\n",
      "9997                          [cupcake_2120, ya, thought]\n",
      "9998                [cupcake_dolli, ye, ye, m, glad, fun]\n",
      "9999                            [cupcake_kayla, haha, ye]\n",
      "Name: SentimentText, Length: 10000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[\"SentimentText\"])\n",
    "print(test_df[\"SentimentText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3BrQ-KNzyRyH"
   },
   "outputs": [],
   "source": [
    "df[\"Sentimentstemmed\"] = df[\"SentimentText\"].apply(lambda x: ' '.join([w for w in x]))\n",
    "test_df[\"SentimentText\"] = test_df[\"SentimentText\"].apply(lambda x: ' '.join([w for w in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1582138728083,
     "user": {
      "displayName": "shubhangi srivastava",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDOH0h-cwBvzv3AHuKWWQdg6Csr1NsYduKRItunaQ=s64",
      "userId": "03789907900117995919"
     },
     "user_tz": 360
    },
    "id": "CS9J85DlyTVi",
    "outputId": "d0da9341-ed46-4e1f-82cc-430a846e54be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       ItemID  Sentiment SentimentSource  \\\n",
      "0           1          0    Sentiment140   \n",
      "1           2          0    Sentiment140   \n",
      "2           3          1    Sentiment140   \n",
      "3           4          0    Sentiment140   \n",
      "4           5          0    Sentiment140   \n",
      "...       ...        ...             ...   \n",
      "89984   89996          1    Sentiment140   \n",
      "89985   89997          1    Sentiment140   \n",
      "89986   89998          1    Sentiment140   \n",
      "89987   89999          0    Sentiment140   \n",
      "89988   90000          0    Sentiment140   \n",
      "\n",
      "                                           SentimentText  \\\n",
      "0                                     [sad, apl, friend]   \n",
      "1                             [miss, new, moon, trailer]   \n",
      "2                                   [omg, alreadi, 7:30]   \n",
      "3      [, omgaga, im, sooo, im, gunna, cri, ve, denti...   \n",
      "4                            [think, mi, bf, cheat, t_t]   \n",
      "...                                                  ...   \n",
      "89984  [clevercatsknit, gnome, hat, problem, finish, ...   \n",
      "89985  [clevercatsknit, saw, linn, bakeri, thought, v...   \n",
      "89986                         [cleverdaisi, would, love]   \n",
      "89987                                  [cleverick, evid]   \n",
      "89988  [cleverindi, spine, thing, sound, good, back, ...   \n",
      "\n",
      "                                        Sentimentstemmed  \n",
      "0                                         sad apl friend  \n",
      "1                                  miss new moon trailer  \n",
      "2                                       omg alreadi 7:30  \n",
      "3       omgaga im sooo im gunna cri ve dentist sinc 1...  \n",
      "4                                  think mi bf cheat t_t  \n",
      "...                                                  ...  \n",
      "89984  clevercatsknit gnome hat problem finish size p...  \n",
      "89985  clevercatsknit saw linn bakeri thought veggi f...  \n",
      "89986                             cleverdaisi would love  \n",
      "89987                                     cleverick evid  \n",
      "89988  cleverindi spine thing sound good back exercis...  \n",
      "\n",
      "[89989 rows x 5 columns]\n",
      "      ItemID  Sentiment SentimentSource  \\\n",
      "0      90001          1    Sentiment140   \n",
      "1      90002          1    Sentiment140   \n",
      "2      90003          1    Sentiment140   \n",
      "3      90004          1    Sentiment140   \n",
      "4      90005          0    Sentiment140   \n",
      "...      ...        ...             ...   \n",
      "9995   99996          0    Sentiment140   \n",
      "9996   99997          1    Sentiment140   \n",
      "9997   99998          0    Sentiment140   \n",
      "9998   99999          1    Sentiment140   \n",
      "9999  100000          1    Sentiment140   \n",
      "\n",
      "                                          SentimentText  \n",
      "0     clevermonkey hit magnolia see anvil stori anvi...  \n",
      "1     clevermonkey seen boot car movi goodth m veget...  \n",
      "2                  clevertitania good morn rain thunder  \n",
      "3                                     clewis4u91 glad m  \n",
      "4                                clexisep mss sad cooki  \n",
      "...                                                 ...  \n",
      "9995  cupcak seem like repeat problem hope re abl fi...  \n",
      "9996  cupcak arrrr repli differ tweet time ll see du...  \n",
      "9997                            cupcake_2120 ya thought  \n",
      "9998                     cupcake_dolli ye ye m glad fun  \n",
      "9999                              cupcake_kayla haha ye  \n",
      "\n",
      "[10000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HGaDwkwL23b4"
   },
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fJhQ-oQj21EJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get_tfidf(train, test):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(max_df = 0.90 , min_df = 10, stop_words = 'english')\n",
    "    tfidf = vectorizer.fit_transform(train).toarray()\n",
    "    tfidf_test = vectorizer.transform(test).toarray()\n",
    "    return tfidf , tfidf_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gW-b5vI_tzk9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score , score_test = get_tfidf(df[\"Sentimentstemmed\"], test_df[\"SentimentText\"] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(score)\n",
    "print(score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 230,
     "status": "error",
     "timestamp": 1582138774507,
     "user": {
      "displayName": "shubhangi srivastava",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDOH0h-cwBvzv3AHuKWWQdg6Csr1NsYduKRItunaQ=s64",
      "userId": "03789907900117995919"
     },
     "user_tz": 360
    },
    "id": "ukJThPTZ0cHA",
    "outputId": "3d9cc993-4845-472b-b55c-8aacbada13b0"
   },
   "outputs": [],
   "source": [
    "X = score\n",
    "y = df[\"Sentiment\"]\n",
    "\n",
    "\n",
    "def sigmoid(X, weight):\n",
    "    z = nm.dot(X, weight)\n",
    "    return 1 / (1 + nm.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 404,
     "status": "error",
     "timestamp": 1582136760229,
     "user": {
      "displayName": "shubhangi srivastava",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDOH0h-cwBvzv3AHuKWWQdg6Csr1NsYduKRItunaQ=s64",
      "userId": "03789907900117995919"
     },
     "user_tz": 360
    },
    "id": "C-Eh4iLI2L9L",
    "outputId": "7eab764d-9c18-4fbb-bdc4-41fd6afff6a0"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, h, y):\n",
    "    return nm.dot(X.T, (h - y)) / y.shape[0]\n",
    "\n",
    "def update_weight_loss(weight, learning_rate, gradient):\n",
    "    return weight - learning_rate * gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(X, y):\n",
    "    num_iter = 100\n",
    "   \n",
    "    theta = nm.zeros(X.shape[1])\n",
    " \n",
    "    for i in range(num_iter):\n",
    "        h = sigmoid(X, theta)\n",
    "        gradient = gradient_descent(X, h, y)\n",
    "        theta = update_weight_loss(theta, 0.1, gradient)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(x, theta):\n",
    "    theta_1 = theta[:, nm.newaxis]\n",
    "    return sigmoid(x,theta_1)\n",
    "\n",
    "def acc(actual, pred):\n",
    "    predicted_class = ((pred >= 0.5) .astype(int))\n",
    "    predicted_class = predicted_class.flatten()\n",
    "    accuracy = nm.mean(predicted_class == actual)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kfold = KFold(10)\n",
    "bestaccuracy = 0\n",
    "theta_2 = nm.zeros(X.shape[1])\n",
    "\n",
    "for train, test in kfold.split(X):\n",
    "    X_train = X[train]\n",
    "    X_validate = X[test]\n",
    "    \n",
    "    Y_train = y[train]\n",
    "    Y_validate = y[test]\n",
    "    \n",
    "    theta_3 = grad(X_train, Y_train)\n",
    "    \n",
    "    pred = predict_test(X_validate, theta_3)\n",
    "    \n",
    "    accuracy = acc(Y_validate, pred)\n",
    "    \n",
    "    if(accuracy > bestaccuracy):\n",
    "        theta_2 = theta_3\n",
    "        bestaccuracy = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.69669714e-05 4.11901541e-04 1.91843575e-04 ... 4.60787387e-05\n",
      " 9.36247465e-05 2.26691401e-04]\n",
      "0.7196355150572286\n"
     ]
    }
   ],
   "source": [
    "print(theta_2)\n",
    "print(bestaccuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final = predict_test(score_test, theta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50058368],\n",
       "       [0.5000251 ],\n",
       "       [0.50322637],\n",
       "       ...,\n",
       "       [0.50115861],\n",
       "       [0.50473716],\n",
       "       [0.50536043]])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7081"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_y = test_df[\"Sentiment\"]\n",
    "\n",
    "accuracy = acc(actual_y, test_final)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average precision-recall score: 0.81\n",
      "0.7062927680128808\n",
      "0.8782115448782115\n"
     ]
    }
   ],
   "source": [
    "precision = average_precision_score(actual_y, test_final)\n",
    "test_final_1 = ((test_final >= 0.5) .astype(int))\n",
    "\n",
    "prec = precision_score(actual_y, test_final_1)\n",
    "recall = recall_score(actual_y, test_final_1)\n",
    "conf = confusion_matrix(actual_y, test_final_1)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(precision))\n",
    "print(prec)\n",
    "print(recall)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
